"""
ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì—:
pip install langchain-text-splitters requests
"""

import json
import os
import re  
from langchain_text_splitters import RecursiveCharacterTextSplitter, Language
import sys 
import requests # <--- ì¶”ê°€: LLM í˜¸ì¶œì„ ìœ„í•´ requests ëª¨ë“ˆ ì¶”ê°€

from core.config import SOLAR_API_KEY
# -----------------------------
# 1. ì„¤ì • (LLM API ì„¤ì • ì¶”ê°€)
# -----------------------------

# [ 1. ì…ë ¥ ] ì²˜ë¦¬í•  ì½”ë“œ íŒŒì¼ ê²½ë¡œ
try:
    file_path = sys.argv[1] 
except IndexError:
    print("ì˜¤ë¥˜: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œë¥¼ ëª…ë ¹ì¤„ ì¸ìë¡œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.", file=sys.stderr)
    sys.exit(1)

# [ 2. ì¶œë ¥ ] ìµœì¢… ê²°ê³¼ íŒŒì¼ ê´€ë ¨ ë³€ìˆ˜ëŠ” ì œê±°
base_name = os.path.basename(file_path)
file_name_without_extension = os.path.splitext(base_name)[0]

MAX_CHUNK_CHAR_LENGTH = 2000 

LANGUAGE_MAP = {
    ".py": Language.PYTHON,
    ".js": Language.JS,
    ".java": Language.JAVA,
    ".c": Language.C,
    ".cpp": Language.CPP,
    ".go": Language.GO,
    ".rb": Language.RUBY,
    ".ts": Language.TS,
}

# API í‚¤ ë° ì—”ë“œí¬ì¸íŠ¸
SOLAR_LLM_ENDPOINT = "https://api.upstage.ai/v1/chat/completions"

# Solar API ì§ì ‘ í˜¸ì¶œì„ ìœ„í•œ í—¤ë”
SOLAR_LLM_HEADERS = {
    "Authorization": f"Bearer {SOLAR_API_KEY}",
    "Content-Type": "application/json"
}

# -----------------------------
# 2. ì½”ë“œ ì „ì²´ ìš”ì•½ í•¨ìˆ˜ (ìƒˆë¡œ ì¶”ê°€)
# -----------------------------

def call_solar_code_summary(file_name: str, full_code: str) -> str:
    """Solar LLMì„ í˜¸ì¶œí•˜ì—¬ ì „ì²´ ì½”ë“œ íŒŒì¼ì„ ìš”ì•½í•©ë‹ˆë‹¤. íŒŒì¼ í¬ê¸°ê°€ í´ ê²½ìš° ì¤‘ê°„ ë¶€ë¶„ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤."""
    
    # LLM í”„ë¡¬í”„íŠ¸ì— ë“¤ì–´ê°ˆ ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´ (10,000ì ì œí•œ)
    MAX_PROMPT_TEXT_LENGTH = 10000
    code_text = full_code
    
    # 1. ê¸¸ì´ ì²´í¬ ë° í”„ë¡¬í”„íŠ¸ ìƒì„±
    if len(code_text) > MAX_PROMPT_TEXT_LENGTH:
        # ì½”ë“œê°€ ë„ˆë¬´ ê¸¸ë©´ ì¤‘ê°„ ë¶€ë¶„ 10,000ìë§Œ ì¶”ì¶œ
        start_index = (len(code_text) - MAX_PROMPT_TEXT_LENGTH) // 2
        end_index = start_index + MAX_PROMPT_TEXT_LENGTH
        
        truncated_content = code_text[start_index:end_index]
        print(f"[LLM] íŒŒì¼ í¬ê¸°ê°€ ì»¤ì„œ ì½”ë“œì˜ ì¤‘ê°„ ë¶€ë¶„ (ì´ {MAX_PROMPT_TEXT_LENGTH}ì)ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.", file=sys.stderr)
        
        prompt = f"""ë‹¤ìŒì€ íŒŒì¼ ì œëª©ê³¼ ì½”ë“œì˜ ì¤‘ê°„ ë¶€ë¶„ì…ë‹ˆë‹¤.
ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ **{file_name}** íŒŒì¼ì´ ì–´ë–¤ ëª©ì ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆëŠ”ì§€, ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ì§€ 3ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ ì£¼ì„¸ìš”.
ì½”ë“œì˜ ì‹œì‘ê³¼ ëì´ ì•„ë‹ˆë©° ì¤‘ê°„ ë¶€ë¶„ë§Œ í¬í•¨ë˜ì–´ ìˆìŒì„ ê³ ë ¤í•´ ì£¼ì„¸ìš”.

[ì½”ë“œ ì •ë³´]
---
íŒŒì¼ ì œëª©: {file_name}
ë‚´ìš©:
{truncated_content}
---

ì½”ë“œ ì „ì²´ ìš”ì•½:"""
    else:
        # ì „ì²´ ì½”ë“œ ì‚¬ìš©
        prompt = f"""ë‹¤ìŒì€ íŒŒì¼ ì œëª©ê³¼ ì½”ë“œì˜ ì „ì²´ ë‚´ìš©ì…ë‹ˆë‹¤. 
ì´ **{file_name}** íŒŒì¼ì´ ì–´ë–¤ ëª©ì ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆëŠ”ì§€, ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ì§€ 3ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ ì£¼ì„¸ìš”.

[ì½”ë“œ ì „ì²´ ë‚´ìš©]
---
íŒŒì¼ ì œëª©: {file_name}
ë‚´ìš©:
{code_text}
---

ì½”ë“œ ì „ì²´ ìš”ì•½:"""
        
    # 2. LLM í˜¸ì¶œ
    payload = {
        "model": "solar-pro2", 
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.1,
        "max_tokens": 512
    }
    
    try:
        print(f"[LLM] ì½”ë“œ ì „ì²´ ìš”ì•½ ìš”ì²­ ì¤‘...", file=sys.stderr)
        response = requests.post(SOLAR_LLM_ENDPOINT, headers=SOLAR_LLM_HEADERS, json=payload, timeout=60)
        response.raise_for_status()
        
        response_json = response.json()
        summary = response_json['choices'][0]['message']['content'].strip()
        print(f"[LLM] ì½”ë“œ ìš”ì•½ ì™„ë£Œ. (ìš”ì•½ ê¸¸ì´: {len(summary)}ì)", file=sys.stderr)
        return summary
        
    except (requests.exceptions.RequestException, KeyError, IndexError) as e:
        print(f"[LLM ìš”ì•½ ì˜¤ë¥˜] ìš”ì²­ ì‹¤íŒ¨ ë˜ëŠ” ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: {e}", file=sys.stderr)
        return f"[ì½”ë“œ ìš”ì•½ ì‹¤íŒ¨: {file_name}]"


# -----------------------------
# 3. ì½”ë“œ íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜ (ìˆ˜ì •: ìš”ì•½ ìƒì„± ë¡œì§ ì¶”ê°€)
# -----------------------------
def process_code_file(input_file, doc_id, file_name_prefix):
    
    file_extension = os.path.splitext(input_file)[1].lower()
    
    if file_extension not in LANGUAGE_MAP:
        print(f"ì˜¤ë¥˜: '{file_extension}'ì€(ëŠ”) ì§€ì›í•˜ëŠ” ì½”ë“œ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.", file=sys.stderr)
        return

    try:
        with open(input_file, "r", encoding="utf-8") as f:
            full_code = f.read()
    except Exception as e:
        print(f"íŒŒì¼ ì½ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", file=sys.stderr)
        return

    print(f"\n[ì½”ë“œ ì²˜ë¦¬] '{input_file}' íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ (ì–¸ì–´: {LANGUAGE_MAP[file_extension].value})...", file=sys.stderr)

    # ğŸŒŸ 1. ë¬¸ì„œ ì „ì²´ ìš”ì•½ ìƒì„± (LLM í˜¸ì¶œ) ğŸŒŸ
    document_summary = call_solar_code_summary(file_name_prefix, full_code)
    
    # 2. Langchain ì–¸ì–´ë³„ ë¶„í• ê¸°ë¥¼ ì‚¬ìš© (ê¸°ì¡´ ë¡œì§)
    language_enum = LANGUAGE_MAP[file_extension]
    text_splitter = RecursiveCharacterTextSplitter.from_language(
        language=language_enum,
        chunk_size=MAX_CHUNK_CHAR_LENGTH,
        chunk_overlap=200,
        length_function=len
    )
    
    # 3. ì½”ë“œë¥¼ ë¶„í• 
    code_chunks = text_splitter.split_text(full_code)
    
    if not code_chunks:
        print("íŒŒì¼ ë‚´ìš©ì´ ë¹„ì–´ìˆì–´ ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.", file=sys.stderr)
        return

    print(f"  > ì½”ë“œë¥¼ ì´ {len(code_chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤. (í›„ì²˜ë¦¬ ì‹œì‘...)", file=sys.stderr)

    # 4. ìµœì¢… JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (íŒŒì¼ ì œëª© ë° summary í¬í•¨)
    final_chunks_for_embedding = []
    for i, chunk_text in enumerate(code_chunks):
        
        # --- [ê¸°ì¡´ ë¡œì§: í›„ì²˜ë¦¬(Post-processing)] ---
        cleaned_chunk = re.sub(r'\n\s*\n+', '\n', chunk_text)
        cleaned_chunk = re.sub(r' {2,}', ' ', cleaned_chunk)
        # --- [ìˆ˜ì • ì¢…ë£Œ] ---

        final_text_to_embed = f"íŒŒì¼ ì œëª©: {file_name_prefix}\n\nì½”ë“œ ë‚´ìš©:\n{cleaned_chunk}" 
        
        final_chunks_for_embedding.append({
            "doc_id": doc_id,
            "page": 1,
            "chunk_in_page": i,
            "text_for_embedding": final_text_to_embed,
            "summary": document_summary # <--- ğŸ’¡ ë¬¸ì„œ ì „ì²´ ìš”ì•½ ì¶”ê°€
        })

    # <--- í•µì‹¬ ìˆ˜ì •: ìµœì¢… ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ JSON ë¬¸ìì—´ë¡œ stdoutì— ì¶œë ¥
    print(json.dumps(final_chunks_for_embedding, ensure_ascii=False))
    
    # ë¡œê·¸ëŠ” stderrë¡œ ì¶œë ¥
    print(f"\n[ì½”ë“œ ì²˜ë¦¬] ìµœì¢… ì²­í‚¹ ì™„ë£Œ! (í›„ì²˜ë¦¬ ì ìš©) {len(code_chunks)}ê°œ ì²­í¬ ìƒì„±.", file=sys.stderr)

# -----------------------------
# 4. ì‹¤í–‰ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
# -----------------------------
if __name__ == "__main__":
    try:
        if not os.path.exists(file_path):
            print(f"ì˜¤ë¥˜: ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}", file=sys.stderr)
            sys.exit(1)
        else:
            absolute_path = os.path.abspath(file_path)

            process_code_file(
                input_file=file_path,
                doc_id=absolute_path,
                file_name_prefix=file_name_without_extension
            )
            print("\n--- ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„±ê³µ ---", file=sys.stderr)

    except Exception as e:
        print(f"\n--- íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ---", file=sys.stderr)
        print(e, file=sys.stderr)
        sys.exit(1)